# -*- coding: utf-8 -*-
"""Untitled52.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-KODL21IYIOdQaNnoVJuEx-86u1_IH7B
"""

pip install selenium pandas

#必要なものをインストール（最初の1回だけでOK）
!apt-get update
!apt install -y chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin

import csv
import re
import logging
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def setup_logger():
    logging.basicConfig(
        filename='scraping.log',
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

def is_news_like(text):
    """ニュースっぽいテキストをフィルター"""
    if len(text) < 20:
        return False
    jp_chars = re.findall(r'[ぁ-んァ-ン一-龥]', text)
    if len(jp_chars) / len(text) < 0.5:
        return False
    if not re.search(r'[。、！？「」]', text):
        return False
    ng_words = ['続きを読む', 'もっと見る', '画像を見る']
    if any(word in text for word in ng_words):
        return False
    return True

def get_news_titles(driver, url):
    driver.get(url)

    # ページが完全に読み込まれるまで待機
    WebDriverWait(driver, 10).until(
        EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a"))
    )

    # BeautifulSoupでパース
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    all_links = soup.find_all('a')

    titles = []
    for a in all_links:
        text = a.get_text(strip=True)
        if is_news_like(text):
            titles.append(text)

    return list(set(titles))  # 重複排除

def save_to_csv(titles, filename='news_titles.csv'):
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['No', 'Title', 'Scraped At'])
        for i, title in enumerate(titles, 1):
            writer.writerow([i, title, now])

def main():
    setup_logger()
    logging.info('スクレイピング開始')

    url = "https://news.yahoo.co.jp/"

    # Seleniumセットアップ（ユーザーディレクトリ指定なし！）
    options = Options()
    options.add_argument('--headless')  # ヘッドレスでもOK
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)

    try:
        titles = get_news_titles(driver, url)
        save_to_csv(titles)
        logging.info(f'{len(titles)} 件のニュースタイトルを保存しました')
    except Exception as e:
        logging.error(f'エラー発生: {e}')
    finally:
        driver.quit()
        logging.info('スクレイピング完了')

if __name__ == "__main__":
    main()